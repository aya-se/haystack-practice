{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hattori-k/.conda/envs/qa/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# FAISSによるDocument Store\n",
    "from haystack.utils import clean_wiki_text, convert_files_to_docs, fetch_archive_from_http, print_answers\n",
    "from haystack.nodes import FARMReader, TransformersReader\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "\n",
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing Documents: 10000it [00:02, 3458.35it/s]            \n"
     ]
    }
   ],
   "source": [
    "# サンプルデータでドキュメントを作成\n",
    "doc_dir = \"data/tutorial6\"\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt6.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
    "\n",
    "docs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 737/737 [00:00<00:00, 296kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 58.8kB/s]\n",
      "Downloading: 100%|██████████| 8.65k/8.65k [00:00<00:00, 2.46MB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 266kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 45.6kB/s]\n",
      "Downloading: 100%|██████████| 25.5k/25.5k [00:00<00:00, 6.65MB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [00:05<00:00, 73.6MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 20.7kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 112kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 569kB/s]  \n",
      "Downloading: 100%|██████████| 363/363 [00:00<00:00, 166kB/s]\n",
      "Downloading: 100%|██████████| 13.9k/13.9k [00:00<00:00, 89.4kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 351kB/s]  \n",
      "Downloading: 100%|██████████| 229/229 [00:00<00:00, 107kB/s]\n",
      "WARNING:haystack.nodes.retriever._embedding_encoder:You are using a Sentence Transformer with the dot_product function. We recommend using cosine instead. This can be set when initializing the DocumentStore\n",
      "Batches: 100%|██████████| 74/74 [00:13<00:00,  5.39it/s]docs/s]\n",
      "Documents Processed: 10000 docs [00:15, 625.19 docs/s]          \n"
     ]
    }
   ],
   "source": [
    "# EmbeddingによるRetriever\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    model_format=\"sentence_transformers\",\n",
    ")\n",
    "\n",
    "document_store.update_embeddings(retriever) # 埋め込み表現の更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# FARMReader\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パイプラインの作成\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.44it/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 45.60 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 60.26 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 99.95 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 77.05 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 99.59 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 99.33 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 99.09 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Who created the Dothraki vocabulary?\n",
      "Answers:\n",
      "[   {   'answer': 'David J. Peterson',\n",
      "        'context': 'orld. The language was developed for the TV series by the '\n",
      "                   'linguist David J. Peterson, working off the Dothraki words '\n",
      "                   \"and phrases in Martin's novels.\\n\"\n",
      "                   ','},\n",
      "    {   'answer': 'David J. Peterson',\n",
      "        'context': '\\n'\n",
      "                   '===Valyrian===\\n'\n",
      "                   'David J. Peterson, who created the Dothraki language for '\n",
      "                   'the first season of the show, was entrusted by the '\n",
      "                   'producers to design a new '},\n",
      "    {   'answer': 'David J. Peterson',\n",
      "        'context': \"age for ''Game of Thrones''\\n\"\n",
      "                   'The Dothraki vocabulary was created by David J. Peterson '\n",
      "                   'well in advance of the adaptation. HBO hired the Language '\n",
      "                   'Creatio'},\n",
      "    {   'answer': 'Dwight Schrute',\n",
      "        'context': '\\'s Ancestry\" from the United States television show '\n",
      "                   \"''The Office'', Dwight Schrute created the Dothraki phrase \"\n",
      "                   '\"throat rip\" by placing \"throat\" in fro'},\n",
      "    {   'answer': 'SVO',\n",
      "        'context': '\\n'\n",
      "                   '===Word order===\\n'\n",
      "                   'The basic word order is SVO (subject–verb–object). In a '\n",
      "                   'basic sentence, the order of these elements (when all '\n",
      "                   'three are present) is '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# パイプラインによる質問のサンプル実行\n",
    "prediction = pipe.run(\n",
    "    query=\"Who created the Dothraki vocabulary?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")\n",
    "print_answers(prediction, details=\"minimum\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69f900a3965c754a26eb9eb820a3df98862b428c61ff66dc953c543ac310815b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('qa': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
